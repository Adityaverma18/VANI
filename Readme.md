ğŸ“Œ VANI â€“ Real-Time Speech Emotion & Speaker Recognition System.
ğŸ™ï¸ Emotion Detection + Speaker Identification (Dual Deep Learning System).
ğŸ§  Project Overview.

VANI is a real-time Speech Emotion Recognition (SER) and Speaker Recognition system built using Deep Learning.
The system captures live audio, extracts MFCC features, and simultaneously predicts:

ğŸ­ Emotion (Happy, Sad, Angry, Fear, Neutral, Disgust, Surprise).

ğŸ—£ï¸ Speaker Identity (Unique voice-based recognition).

VANI aims to assist individuals with Attention Deficit, increase emotional awareness, and support interactive intelligent systems.

ğŸš€ Key Features

âœ” Real-time emotion & speaker prediction.
âœ” Dual-model architecture (Emotion + Speaker).
âœ” MFCC feature extraction.
âœ” Conv1D + BiLSTM + Attention Network.
âœ” Live microphone streaming using sounddevice.
âœ” Dataset automation with KaggleHub.
âœ” High accuracy with augmentation, cosine LR annealing, early stopping.
âœ” Scalable, modular project structure.
âœ” Train your own models or use pre-trained ones.

ğŸ“‚ Project Structure

```
VANI/
â”‚â”€â”€ main.py
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ data_path.csv
â”‚â”€â”€ speaker_data.csv
â”‚
â”œâ”€â”€ models/
â”‚    â””â”€â”€ README.md  (placeholder, models not uploaded)
â”‚
â””â”€â”€ src/
     â”œâ”€â”€ emotion_data_extraction.py
     â”œâ”€â”€ voice_data_seperation.py
     â”œâ”€â”€ features.py
     â”œâ”€â”€ model_emotion.py
     â”œâ”€â”€ model_speaker.py
     â”œâ”€â”€ train_emotion.py
     â”œâ”€â”€ train_speaker.py
     â”œâ”€â”€ predict_emotion.py
     â”œâ”€â”€ predict_speaker.py
     â”œâ”€â”€ utils.py
     â””â”€â”€ __init__.py
```


ğŸ“Š Datasets Used
Emotion Recognition
âœ” RAVDESS.
âœ” TESS.
âœ” CREMA-D.
âœ” SAVEE.

Speaker Recognition
âœ” Speaker Recognition Audio Dataset (Kaggle).
âœ” VoxCeleb-style structured dataset.

Automatic download using:

```
import kagglehub
dataset_path = kagglehub.dataset_download("dataset/name")
```

ğŸ› ï¸ Technologies & Tools
Machine Learning / Deep Learning

TensorFlow / Keras

Conv1D, BiLSTM, Attention

MFCC Extraction (Librosa)

Cosine Annealing LR Scheduler

Early Stopping, Model Checkpoint

Audio Processing

Librosa

SoundDevice

NumPy

Data Science

Pandas

Scikit-Learn

Matplotlib

ğŸ§© Methodology
1ï¸âƒ£ Audio Input

Live microphone audio

3-second sampling

Normalized to target sample rate

2ï¸âƒ£ Feature Extraction

Convert raw waveform â†’ MFCC

Padding / Truncating length

Augmentation:
âœ” Noise injection
âœ” Time stretch
âœ” Pitch shift

3ï¸âƒ£ Model Processing
ğŸ­ Emotion Model

Conv1D (feature extraction)

BiLSTM (temporal learning)

Attention (focus on strong features)

Softmax prediction

ğŸ—£ï¸ Speaker Model

Conv1D + LSTM composite

Optimized with class balancing

4ï¸âƒ£ Output

```
Emotion: Happy (0.87 confidence)
Speaker: User_03 (0.94 confidence)
```

5ï¸âƒ£ Real-time Loop

System continuously records â†’ processes â†’ displays â†’ repeats.

ğŸ–¥ï¸ Installation
Clone the Repo

```
git clone https://github.com/yourname/VANI.git
cd VANI
```

Install Dependencies
```
pip install -r requirements.txt
```

ğŸ“ Training Emotion Model
```
python src/train_emotion.py --data_csv data_path.csv --out_dir models --epochs 60 --batch_size 32
```

ğŸ“ Training Speaker Model
```
python src/train_speaker.py --data_csv speaker_data.csv --out_dir models --max_len 130
```

ğŸ¤ Run Real-Time System
```
python main.py --max_len 130 \
 --emotion_model models/emotion_model_final.keras \
 --speaker_model models/speaker_model_final.keras
```

ğŸ“ˆ Results & Visualizations
ğŸ“‰ Loss Graph

(Generated during training)

ğŸ“ˆ Accuracy Graph

(Training vs Validation)

ğŸ” Confusion Matrix

Emotion recognition performance per class

Speaker identification clarity

ğŸ“¦ models/ Folder (Important)

Do NOT upload large model files.
Include only this:

models/
â””â”€â”€ README.md


With contents explaining how to download or train models.

ğŸ¤ Contributing

Pull requests are welcome!
Suggestions for model improvement or dataset expansion are encouraged.

ğŸ“œ License

MIT License (recommended)

ğŸ§  Conclusion

VANI successfully integrates dual deep learning pipelines for real-time:

Emotion Detection

Speaker Recognition

This system bridges ML and human interaction, enabling smarter emotional understanding and adaptive communication for real-world applications.
