
# ğŸ“Œ VANI â€“ Real-Time Speech Emotion & Speaker Recognition System
### ğŸ™ï¸ Dual Deep Learning System: Emotion Detection + Speaker Identification

## ğŸ§  Project Overview
VANI is a real-time deep learning system capable of recognizing **human emotions** and **speaker identity** from live audio.
It uses **MFCC-based audio processing**, **CNN + BiLSTM models**, and **real-time microphone streaming** to deliver fast, accurate predictions.

The system supports:
- Individuals with Attention Deficiency  
- Emotion-aware AI systems  
- Smart assistants and HCI applications

VANI predicts:
- ğŸ­ Emotion (Happy, Sad, Angry, Fear, Neutral, Disgust, Surprise)
- ğŸ—£ï¸ Speaker Identity (Who is speaking)

---

## ğŸš€ Key Features
- Real-time emotion & speaker prediction  
- MFCC feature extraction  
- Conv1D + BiLSTM + Attention  
- Live audio streaming using sounddevice  
- Automatic dataset download via KaggleHub  
- High accuracy with augmentation  
- Cosine LR annealing & early stopping  
- Modular project structure  

---

## ğŸ“‚ Project Structure
VANI/
```
â”‚â”€â”€ main.py  
â”‚â”€â”€ README.md  
â”‚â”€â”€ requirements.txt  
â”‚â”€â”€ data_path.csv  
â”‚â”€â”€ speaker_data.csv  
â”‚  
â”œâ”€â”€ models/  
â”‚    â””â”€â”€ README.md  (models not uploaded)  
â”‚  
â””â”€â”€ src/  
     â”œâ”€â”€ emotion_data_extraction.py  
     â”œâ”€â”€ voice_data_seperation.py  
     â”œâ”€â”€ features.py  
     â”œâ”€â”€ model_emotion.py  
     â”œâ”€â”€ model_speaker.py  
     â”œâ”€â”€ train_emotion.py  
     â”œâ”€â”€ train_speaker.py  
     â”œâ”€â”€ predict_emotion.py  
     â”œâ”€â”€ predict_speaker.py  
     â”œâ”€â”€ utils.py  
     â””â”€â”€ __init__.py  
```
---

## ğŸ“Š Datasets Used
Emotion:
- RAVDESS  
- TESS  
- CREMA-D  
- SAVEE  

Speaker:
- Kaggle Speaker Recognition Dataset  
- Speaker Recognition Audio Dataset  

Download:
```
import kagglehub
path = kagglehub.dataset_download("dataset/name")
```

---

## ğŸ› ï¸ Technologies Used
- TensorFlow / Keras  
- MFCC (Librosa)  
- Conv1D, BiLSTM, Attention  
- SoundDevice  
- NumPy / Pandas  
- Scikit-Learn  
- Matplotlib  

---

## ğŸ§© Methodology
### 1ï¸âƒ£ Audio Input
- Microphone captures 3s audio  
- Resampled & normalized  

### 2ï¸âƒ£ Feature Extraction
- MFCC generation  
- Padding/truncation  
- Augmentation: noise, pitch shift, time-stretch  

### 3ï¸âƒ£ Model Processing
- Emotion: Conv1D + BiLSTM + Attention  
- Speaker: Conv1D + LSTM  
- Softmax classification  

### 4ï¸âƒ£ Output Example
Emotion: Happy (0.87)  
Speaker: User_03 (0.94)

### 5ï¸âƒ£ Real-Time Loop
Record â†’ MFCC â†’ Emotion Model â†’ Speaker Model â†’ Display â†’ Repeat

---

## ğŸ–¥ï¸ Installation
```
git clone https://github.com/yourname/VANI.git
cd VANI
pip install -r requirements.txt
```

---

## ğŸ“ Training
Emotion:
```
python src/train_emotion.py --data_csv data_path.csv --out_dir models
```

Speaker:
```
python src/train_speaker.py --data_csv speaker_data.csv --out_dir models
```

---

## ğŸ¤ Real-Time Execution
```
python main.py --max_len 130 --emotion_model models/emotion_model_final.keras --speaker_model models/speaker_model_final.keras
```

---

## ğŸ“ˆ Results
- Loss curve  
- Accuracy curve  
- Confusion matrix  

Generated during training.

---

## ğŸ“¦ models/ Folder
Do NOT upload `.keras` or `.h5` files.

Include only:

```
models/  
â””â”€â”€ README.md  
```

---

## ğŸ¤ Contributing
Pull requests welcome.

## ğŸ“œ License
MIT License

## ğŸ§  Conclusion
VANI unifies **emotion detection** and **speaker recognition** in real time using modern deep learning, MFCC processing, and robust audio modeling.

